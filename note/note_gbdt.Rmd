---
title: "Note of Gradient Boosting Decision Tree"
author: Jiaying Wu
output: html_notebook
---

### Overview

In this note, first we will talk about the general framework of Gradient Boosting Decision Tree (GBDT), then we will focus on these 3 Gradient Boosting Decision Tree algorithms:

* [XGBoost](https://xgboost.readthedocs.io/en/latest/)

* [LightGBM](https://lightgbm.readthedocs.io/en/latest/)

* [CatBoost](https://catboost.ai)


### What is Gradient Boosting Decision Tree?

Gradient Boosting Decision Tree is an additive model,  the prediction of each observation in this model is the sum of each belonging gain of $D$ depths in $T$ trees.

**As the predicted value of each observation start from 0, and each tree have the same depths (Not pruned GBDT). In the pruned tree, the depth of end leaf might different.** 

The model can written as:

$$
\hat{y_{i}} = 0 + \sum^{T}_{t = 1} f_{t}(x_{i}) = \sum^{T}_{t = 1} f_{t}(x_{i}) = \sum^{T}_{t = 1} \sum^{D}_{d = 1} gain_{\text{{t, d, l}}} \\
\text{where} \space\space l \in \{1, 2, ..., 2^d \}, \space gain_{\text{{t, d, l}}} \space \text{might be 0 in a pruned tree}
$$

$\hat{y_{i}}$ is the predicted value of observation $i$.

$x_{i}$ is the given information of observation $i$.

$f_{t}$ is the $t_{th}$ decision tree.

$f_{t}(x_{i})$ is sum of gain in $t_{th}$ decision tree.

In $gain_{\text{{t, d, l}}}$, $t$ determine the $t_{th}$ tree, $d$ determine the $d_{th}$ depth, $l$ determine the $l_{th}$ leaf.

In each depth, one observation can only have one gain value. Hence, the predicted value in **not pruned GBDT** of observation $i$ is the sum of the $(T*D)$ gain value.

**Pruned means we prune the leaves of GBDT from bottom to top of each tree.**

![](http://arogozhnikov.github.io/images/gbdt_attractive_picture.png)

(Image 1 from [Gradient Boosting explained demonstration](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html))

**The image 1 is the pruned Gradient Boosting Decison Tree.**

The image 1 shown, observations $i$ end up with {tree 1, depth 3, leaf 2}, end up with {tree 2, depth 2, leaf 3} ..., and end up with {tree K, depth 2, leaf 2}. Assume we start from 0 of all observations, the predicted value of observation $i$ is,

$$
\begin{align*}
\hat{y_{i}} &=  0 +(gain_{\text{{1, 1, 1}}} + gain_{\text{{1, 2, 1}}} + gain_{\text{{1, 3, 2}}}) + (gain_{\text{{2, 1, 2}}} + gain_{\text{{2, 2, 3}}}) + ... + (gain_{\text{{K, 1, 1}}} +gain_{\text{{K, 2, 2}}}) \\ 
&= f_{1}(x_{i}) + f_{2}(x_{i}) + ... + f_{K}(x_{i})
\end{align*}
$$

In $gain_{\text{{t, d, l}}}$, $t$ determine the $t_{th}$ tree, $d$ determine the $d_{th}$ depth, $l$ determine the $l_{th}$ leaf.

The possible number of leaf in depth $d$ is $2^{d}$.

![](https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/twocart.png)

(Image 2 from dmlc/xgboost)

As image 2 shown, the boy end up with {tree 1, depth 1, leaf 1} and {tree 2, depth 1, leaf 1}, the predicted value is,

$$
\hat{y_{boy}} = gain_{\text{{1, 1, 1}}} + gain_{\text{{2, 1, 1}}} = 2 + 0.9 = 2.9
$$

The grandpa end up with {tree 1, depth 1, leaf 2} and {tree 2, depth 1, leaf 2}, the predicted value is,

$$
\hat{y_{grandpa}} = gain_{\text{{1, 1, 2}}} + gain_{\text{{2, 1, 2}}} = -1 - 0.9 = 1.9
$$

The girl in pink dress end up with {tree 1, depth 1, leaf 1} and {tree 2, depth 1, leaf 2}, the predicted value is,

$$
\hat{y_{girl}} = gain_{\text{{1, 1, 1}}} + gain_{\text{{2, 1, 2}}} = 2 - 0.9 = 1.1
$$


### How Gradient Boosting Decision Tree work?

Assuming we measure the loss using **total sum squared error**, our **goal** is to **minimize** the total sum squared error.

$$
Loss = \sum^{n}_{i = 1} (\hat{y_{i}} - y_{i})^2
$$

#### Express obervation as residual in GBDT

Therefore, the loss in each leaf is the sum squared residuals. 

**As the predicted value start from 0,**

$$
Loss_{1, 0, 1} = \sum^{n}_{i = 1} (\hat{y_{i}} - y_{i})^2 = \sum^{n}_{i = 1} (0 - y_{i})^2 = \sum^{n}_{i = 1} (- y_{i})^2 = \sum^{n}_{i = 1} (Residual_{i, \space \text{{1,0,1}}})^2 \\
\text{where} \space\space \mathbb{E}(Residual_{i \text{{1,0,1}}}) \neq 0
$$

$Loss_{1, 0, 1}$ determine the loss in {tree 1, depth 0, leaf 1}.

$Residual_{i \text{{1,0,1}}}$ determine the residual of observation $i$ as in {tree 1, depth 0, leaf 1}.

As the predicted value start from 0, the ***residuals is the negative of label in the initial leaf of first tree***.

Hence, we can **express the observation in each leaf as residual** rather than actual value. 

It's easier to compute the loss, because **the squared residual is the individual loss of observation in each tree**.


#### How to split the leaves?

At each node, the model trying to **find the best split** that minimize the sum of loss in the left leaf and right leaf.

![](https://www.salford-systems.com/blog/images/site/shapetrees1.jpg)

(Image 3 from salford systems)

The total loss in splited leaves is:

$$
Loss_{\text{{t, (d+1), Left}}} + Loss_{\text{{t, (d+1), Right}}}  = \sum^{L}_{l = 1} ({Residual}_{\text{{t, d, left}}} - \overline{{Residual}_{\text{{t, d, Left}}}})^2 + \sum^{R}_{r = 1} ({Residual}_{\text{{t, d, right}}} - \overline{{Residual}_{\text{{t, d, Right}}}})^2 \\
\text{where} \space\space \overline{{Residual}_{\text{{t, d, Left}}}} = \frac{1}{L}\sum^{L}_{l = 1} {Residual}_{\text{{t, d, left}}}, \space \overline{{Residual}_{\text{{t, d, Right}}}} = \frac{1}{R}\sum^{R}_{r = 1} {Residual}_{\text{{t, d, right}}}, \space L + R = N
$$

$Loss_{\text{{t, (d+1), Left}}}$ is the loss of the left leaf.

$Loss_{\text{{t, (d+1), Right}}}$ is the loss of the right leaf.

${Residual}_{\text{{t, d, left}}}$ is the individual residual in root node that will be split into the left leaf.

${Residual}_{\text{{t, d, right}}}$ is the individual residual in root node that will be split into the right leaf.

$L$ is the sample size in left leaf.

$R$ is the sample size in right leaf.

$N$ is the sample size in root node.

***The gain in each leaf is the mean of residual that split from root node.***


#### Regularization in leaves split

If we keep growing a deeper tree, the training loss won't converged until go down to 0.

$$
\sum^{L}_{l = 1} ({Residual}_{\text{{t, d, left}}} - \overline{{Residual}_{\text{{t, d, Left}}}})^2 + \sum^{R}_{r = 1} ({Residual}_{\text{{t, d, right}}} - \overline{{Residual}_{\text{{t, d, Right}}}})^2 \leq \sum^{R}_{r = 1} ({Residual}_{\text{{t, d, node}}} - \overline{{Residual}_{\text{{t, d, Node}}}})^2 \\
\text{where} \space\space  \overline{{Residual}_{\text{{t, d, Node}}}} = \frac{1}{N}\sum^{N}_{n = 1} {Residual}_{\text{{t, d, node}}}, L + R = N
$$

${Residual}_{\text{{t, d, node}}}$ is the individual residual in root node.

However, we want to minimize future prediction error, that is minimize the test error. 

Base on the bias and variance trade-off, the bias will decreased and variance increased in a deeper tree. Hence, we need to balanced the bias and variance to minimize the test error.

Consider the training error in GBDT as bias, we can approximate the variancewith regularized term. And the parameters select by cross validation.

Then, the loss function can rewrite as:

$$
\sum^{L}_{l = 1} ({Residual}_{\text{{t, d, left}}} - \overline{{Residual}_{\text{{t, d, Left}}}})^2 + \sum^{R}_{r = 1} ({Residual}_{\text{{t, d, right}}} - \overline{{Residual}_{\text{{t, d, Right}}}})^2 + \Omega
$$

The split criteria is:

$$
\sum^{L}_{l = 1} ({Residual}_{\text{{t, d, left}}} - \overline{{Residual}_{\text{{t, d, Left}}}})^2 + \sum^{R}_{r = 1} ({Residual}_{\text{{t, d, right}}} - \overline{{Residual}_{\text{{t, d, Right}}}})^2 + \Omega < \sum^{R}_{r = 1} ({Residual}_{\text{{t, d, node}}} - \overline{{Residual}_{\text{{t, d, Node}}}})^2
$$



### XGBoost








### LightGBM









### CatBoost










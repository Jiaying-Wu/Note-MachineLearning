---
title: "Feature Engineering"
author: Jiaying Wu
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = TRUE,
  message = FALSE,
  error = FALSE,
  warning = FALSE,
  fig.path = "plot/",
  fig.align = "center"
)

# use python
library(reticulate)
use_python("/usr/local/bin/python")
```

## Overview

This notebook is the note of the book `Feature Engineering for Machine Learning`.

Each chapter of this book addresses one data problem:

* how to represent text data or image data

* how to reduce the dimensionality of autogenerated features

* when and how to normalize

Code examples of this book are given in `Python`:

* `NumPy`: provides numeric vector and matrix operations.

* `Pandas`: provides the DataFrame that is the building block of data science in Python.

* `Scikit-learn` : general-purpose machine learning package with extensive coverage of models and feature transformers.

* `Matplotlib`: and the styling library `Seaborn` provide plotting and visualization support.

```{r install_python_package, eval=FALSE, echo=FALSE}
#py_install("NumPy")
#py_install("Pandas")
#py_install("Scikit-learn")
#py_install("Matplotlib")
#py_install("Seaborn")
```

Content in each chapter:

* Chapter 1: introduces the fundamental concepts in the machine learning pipeline (data,
models, features, etc.).

* Chapter 2: basic feature engineering for **numeric data**: `filtering`, `binning`, `scaling`, `log transforms` and `power transforms` and `interaction` features.

* Chapter 3: feature engineering for **natural text**, exploring techniques like `bag-of-words`, `n-grams`, and `phrase detection`.

* Chapter 4: examines `tf-idf` (term frequency–inverse document frequency) as an example of `feature scaling` and discusses why it works.

* Chapter 5: efficient encoding techniques for **categorical variables**, including `feature hashing` and `bin counting`.

* Chapter 6: `principal component analysis (PCA)`.

* Chapter 7: `k-means` as a featurization technique, concept of `model stacking`.

* Chapter 8: anout **images**, two `manual feature extraction` techniques, `SIFT` and `HOG`.

* Chapter 9: a few different techniques in an **end-to-end example**.


### Chapter 1: The Machine Learning Pipeline

`Data`: observations of real-world phenomena.

* `Wrong data` is the result of a mistake in measurement.

* `Redundant data` contains multiple aspects that convey exactly the same information.

`Mathematical model`: describes the relationships between different aspects of the data.

`Feature`: a numeric representation of raw data.

The right features are relevant to the task at hand and should be easy for the model to ingest.

`Feature engineering`: the process of formulating the most appropriate features given the data, the model, and the task.

![](plot/machine_learning_workflow.png)


### Chapter 2: Fancy Tricks with Simple Numbers

Good features should not only represent salient aspects of the data, but also conform to the assumptions of the model.

Models that are `smooth functions` of input features are sensitive to the scale of the input. Model using Euclidean distance should normalize the features so that the output stays on an expected scale.

Model that are `logical functions` are not sensitive to input feature scale. Decision tree models consist of step functions of input features. Models based on `space-partitioning trees` (decision trees, gradient boosted machines, random forests) are not sensitive to scale.

If the scale of the input grows over time, which is the case if the feature is an accumulated count of some sort—eventually it will grow outside of the range that the tree was trained on. It might be necessary to rescale the inputs periodically.

Taken to an extreme, complex features may themselves be the output of statistical models.

#### Scalars, Vectors, and Spaces

`Scalar`: A single numeric feature.

`Vector`: An ordered list of scalars.

`Space`: Vectors sit within a vector space.











